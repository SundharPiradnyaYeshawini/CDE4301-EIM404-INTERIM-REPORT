<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=c6Fu_cJhJ2j2-ZxDMKauV6VXawJgyb8aFYJr-9wuJZM5KEqTDahEV1ECVSsZOMl9_kmCvy597kDopqFAXvpeaA);.lst-kix_9e1n4ciybo9b-8>li:before{content:"\0025a0   "}.lst-kix_r3ktk4yg7cnc-0>li:before{content:"\0025cf   "}.lst-kix_9e1n4ciybo9b-6>li:before{content:"\0025cf   "}.lst-kix_9e1n4ciybo9b-7>li:before{content:"\0025cb   "}.lst-kix_r3ktk4yg7cnc-2>li:before{content:"\0025a0   "}ol.lst-kix_fu03813x2h7a-5.start{counter-reset:lst-ctn-kix_fu03813x2h7a-5 0}.lst-kix_14dnjwe5a6tx-8>li:before{content:"\0025a0   "}.lst-kix_r3ktk4yg7cnc-1>li:before{content:"\0025cb   "}.lst-kix_r3ktk4yg7cnc-3>li:before{content:"\0025cf   "}.lst-kix_9e1n4ciybo9b-1>li:before{content:"\0025cb   "}.lst-kix_14dnjwe5a6tx-6>li:before{content:"\0025cf   "}.lst-kix_14dnjwe5a6tx-4>li:before{content:"\0025cb   "}.lst-kix_14dnjwe5a6tx-7>li:before{content:"\0025cb   "}.lst-kix_9e1n4ciybo9b-2>li:before{content:"\0025a0   "}.lst-kix_9e1n4ciybo9b-3>li:before{content:"\0025cf   "}.lst-kix_14dnjwe5a6tx-3>li:before{content:"\0025cf   "}.lst-kix_9e1n4ciybo9b-4>li:before{content:"\0025cb   "}.lst-kix_9e1n4ciybo9b-5>li:before{content:"\0025a0   "}.lst-kix_14dnjwe5a6tx-5>li:before{content:"\0025a0   "}ul.lst-kix_a4m4v1rfc2mh-5{list-style-type:none}.lst-kix_lhqp2gpz0fi4-2>li:before{content:"\0025a0   "}.lst-kix_fu03813x2h7a-7>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-7,lower-latin) ". "}ul.lst-kix_a4m4v1rfc2mh-4{list-style-type:none}ul.lst-kix_a4m4v1rfc2mh-3{list-style-type:none}.lst-kix_fu03813x2h7a-8>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-8,lower-roman) ". "}ul.lst-kix_a4m4v1rfc2mh-2{list-style-type:none}ul.lst-kix_a4m4v1rfc2mh-1{list-style-type:none}ul.lst-kix_9e1n4ciybo9b-2{list-style-type:none}ul.lst-kix_a4m4v1rfc2mh-0{list-style-type:none}ul.lst-kix_9e1n4ciybo9b-1{list-style-type:none}.lst-kix_fu03813x2h7a-5>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-5,lower-roman) ". "}.lst-kix_fu03813x2h7a-6>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-6,decimal) ". "}ul.lst-kix_9e1n4ciybo9b-0{list-style-type:none}.lst-kix_lhqp2gpz0fi4-1>li:before{content:"\0025cb   "}.lst-kix_lhqp2gpz0fi4-5>li:before{content:"\0025a0   "}.lst-kix_lhqp2gpz0fi4-6>li:before{content:"\0025cf   "}.lst-kix_fu03813x2h7a-8>li{counter-increment:lst-ctn-kix_fu03813x2h7a-8}.lst-kix_lhqp2gpz0fi4-0>li:before{content:"\0025cf   "}.lst-kix_lhqp2gpz0fi4-8>li:before{content:"\0025a0   "}ul.lst-kix_a4m4v1rfc2mh-8{list-style-type:none}ul.lst-kix_a4m4v1rfc2mh-7{list-style-type:none}.lst-kix_lhqp2gpz0fi4-7>li:before{content:"\0025cb   "}ul.lst-kix_a4m4v1rfc2mh-6{list-style-type:none}.lst-kix_r3ktk4yg7cnc-6>li:before{content:"\0025cf   "}ol.lst-kix_fu03813x2h7a-0.start{counter-reset:lst-ctn-kix_fu03813x2h7a-0 0}.lst-kix_r3ktk4yg7cnc-5>li:before{content:"\0025a0   "}.lst-kix_r3ktk4yg7cnc-7>li:before{content:"\0025cb   "}.lst-kix_r3ktk4yg7cnc-4>li:before{content:"\0025cb   "}.lst-kix_r3ktk4yg7cnc-8>li:before{content:"\0025a0   "}.lst-kix_lhqp2gpz0fi4-4>li:before{content:"\0025cb   "}.lst-kix_lhqp2gpz0fi4-3>li:before{content:"\0025cf   "}.lst-kix_fu03813x2h7a-6>li{counter-increment:lst-ctn-kix_fu03813x2h7a-6}.lst-kix_fu03813x2h7a-0>li{counter-increment:lst-ctn-kix_fu03813x2h7a-0}.lst-kix_fu03813x2h7a-0>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-0,decimal) ". "}.lst-kix_fu03813x2h7a-1>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-1,lower-latin) ". "}.lst-kix_fu03813x2h7a-2>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-2,lower-roman) ". "}.lst-kix_fu03813x2h7a-3>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-3,decimal) ". "}.lst-kix_fu03813x2h7a-4>li:before{content:"" counter(lst-ctn-kix_fu03813x2h7a-4,lower-latin) ". "}ul.lst-kix_x308hham3rf2-7{list-style-type:none}ul.lst-kix_sef5n0d1qmsi-7{list-style-type:none}.lst-kix_x308hham3rf2-3>li:before{content:"\0025cf   "}.lst-kix_x308hham3rf2-4>li:before{content:"\0025cb   "}ul.lst-kix_x308hham3rf2-8{list-style-type:none}ul.lst-kix_sef5n0d1qmsi-8{list-style-type:none}.lst-kix_x308hham3rf2-2>li:before{content:"\0025a0   "}.lst-kix_x308hham3rf2-6>li:before{content:"\0025cf   "}ol.lst-kix_fu03813x2h7a-4.start{counter-reset:lst-ctn-kix_fu03813x2h7a-4 0}.lst-kix_x308hham3rf2-0>li:before{content:"\0025cf   "}.lst-kix_x308hham3rf2-7>li:before{content:"\0025cb   "}.lst-kix_x308hham3rf2-8>li:before{content:"\0025a0   "}ul.lst-kix_sef5n0d1qmsi-0{list-style-type:none}ul.lst-kix_sef5n0d1qmsi-1{list-style-type:none}.lst-kix_14dnjwe5a6tx-0>li:before{content:"\0025cf   "}.lst-kix_14dnjwe5a6tx-2>li:before{content:"\0025a0   "}ul.lst-kix_sef5n0d1qmsi-2{list-style-type:none}ul.lst-kix_sef5n0d1qmsi-3{list-style-type:none}.lst-kix_x308hham3rf2-1>li:before{content:"\0025cb   "}ul.lst-kix_sef5n0d1qmsi-4{list-style-type:none}ul.lst-kix_sef5n0d1qmsi-5{list-style-type:none}.lst-kix_14dnjwe5a6tx-1>li:before{content:"\0025cb   "}ul.lst-kix_sef5n0d1qmsi-6{list-style-type:none}.lst-kix_x308hham3rf2-5>li:before{content:"\0025a0   "}.lst-kix_sef5n0d1qmsi-6>li:before{content:"\0025cf   "}ul.lst-kix_9pct4kyjtlro-6{list-style-type:none}ul.lst-kix_9pct4kyjtlro-7{list-style-type:none}.lst-kix_sef5n0d1qmsi-5>li:before{content:"\0025a0   "}.lst-kix_sef5n0d1qmsi-7>li:before{content:"\0025cb   "}ul.lst-kix_9pct4kyjtlro-4{list-style-type:none}ul.lst-kix_9pct4kyjtlro-5{list-style-type:none}.lst-kix_sef5n0d1qmsi-4>li:before{content:"\0025cb   "}.lst-kix_sef5n0d1qmsi-8>li:before{content:"\0025a0   "}.lst-kix_fu03813x2h7a-3>li{counter-increment:lst-ctn-kix_fu03813x2h7a-3}ul.lst-kix_9pct4kyjtlro-8{list-style-type:none}ul.lst-kix_lhqp2gpz0fi4-5{list-style-type:none}ul.lst-kix_lhqp2gpz0fi4-4{list-style-type:none}ul.lst-kix_lhqp2gpz0fi4-3{list-style-type:none}ul.lst-kix_lhqp2gpz0fi4-2{list-style-type:none}ul.lst-kix_9pct4kyjtlro-2{list-style-type:none}ul.lst-kix_lhqp2gpz0fi4-1{list-style-type:none}ol.lst-kix_fu03813x2h7a-1.start{counter-reset:lst-ctn-kix_fu03813x2h7a-1 0}ul.lst-kix_9pct4kyjtlro-3{list-style-type:none}ul.lst-kix_lhqp2gpz0fi4-0{list-style-type:none}ul.lst-kix_9pct4kyjtlro-0{list-style-type:none}ul.lst-kix_9pct4kyjtlro-1{list-style-type:none}.lst-kix_9pct4kyjtlro-8>li:before{content:"\0025a0   "}.lst-kix_sef5n0d1qmsi-0>li:before{content:"\0025cf   "}.lst-kix_9pct4kyjtlro-7>li:before{content:"\0025cb   "}.lst-kix_sef5n0d1qmsi-2>li:before{content:"\0025a0   "}.lst-kix_9pct4kyjtlro-4>li:before{content:"\0025cb   "}ul.lst-kix_x308hham3rf2-0{list-style-type:none}ul.lst-kix_x308hham3rf2-1{list-style-type:none}.lst-kix_sef5n0d1qmsi-1>li:before{content:"\0025cb   "}.lst-kix_sef5n0d1qmsi-3>li:before{content:"\0025cf   "}ul.lst-kix_x308hham3rf2-2{list-style-type:none}.lst-kix_a4m4v1rfc2mh-0>li:before{content:"\0025cf   "}.lst-kix_a4m4v1rfc2mh-2>li:before{content:"\0025a0   "}ul.lst-kix_x308hham3rf2-3{list-style-type:none}.lst-kix_9pct4kyjtlro-6>li:before{content:"\0025cf   "}ul.lst-kix_x308hham3rf2-4{list-style-type:none}ul.lst-kix_x308hham3rf2-5{list-style-type:none}.lst-kix_9pct4kyjtlro-5>li:before{content:"\0025a0   "}ul.lst-kix_x308hham3rf2-6{list-style-type:none}.lst-kix_a4m4v1rfc2mh-1>li:before{content:"\0025cb   "}ul.lst-kix_r3ktk4yg7cnc-8{list-style-type:none}.lst-kix_9pct4kyjtlro-0>li:before{content:"\0025cf   "}ul.lst-kix_r3ktk4yg7cnc-7{list-style-type:none}.lst-kix_a4m4v1rfc2mh-4>li:before{content:"\0025cb   "}.lst-kix_a4m4v1rfc2mh-6>li:before{content:"\0025cf   "}.lst-kix_9pct4kyjtlro-3>li:before{content:"\0025cf   "}.lst-kix_a4m4v1rfc2mh-3>li:before{content:"\0025cf   "}.lst-kix_a4m4v1rfc2mh-7>li:before{content:"\0025cb   "}ul.lst-kix_r3ktk4yg7cnc-0{list-style-type:none}.lst-kix_fu03813x2h7a-5>li{counter-increment:lst-ctn-kix_fu03813x2h7a-5}ul.lst-kix_r3ktk4yg7cnc-2{list-style-type:none}.lst-kix_fu03813x2h7a-2>li{counter-increment:lst-ctn-kix_fu03813x2h7a-2}ul.lst-kix_r3ktk4yg7cnc-1{list-style-type:none}ul.lst-kix_r3ktk4yg7cnc-4{list-style-type:none}.lst-kix_9pct4kyjtlro-2>li:before{content:"\0025a0   "}ul.lst-kix_r3ktk4yg7cnc-3{list-style-type:none}ul.lst-kix_r3ktk4yg7cnc-6{list-style-type:none}.lst-kix_9pct4kyjtlro-1>li:before{content:"\0025cb   "}ul.lst-kix_r3ktk4yg7cnc-5{list-style-type:none}.lst-kix_a4m4v1rfc2mh-5>li:before{content:"\0025a0   "}ol.lst-kix_fu03813x2h7a-6.start{counter-reset:lst-ctn-kix_fu03813x2h7a-6 0}ul.lst-kix_lhqp2gpz0fi4-8{list-style-type:none}ul.lst-kix_lhqp2gpz0fi4-7{list-style-type:none}ul.lst-kix_lhqp2gpz0fi4-6{list-style-type:none}.lst-kix_a4m4v1rfc2mh-8>li:before{content:"\0025a0   "}ol.lst-kix_fu03813x2h7a-3.start{counter-reset:lst-ctn-kix_fu03813x2h7a-3 0}ol.lst-kix_fu03813x2h7a-8{list-style-type:none}ol.lst-kix_fu03813x2h7a-7{list-style-type:none}ol.lst-kix_fu03813x2h7a-7.start{counter-reset:lst-ctn-kix_fu03813x2h7a-7 0}ul.lst-kix_9e1n4ciybo9b-6{list-style-type:none}ol.lst-kix_fu03813x2h7a-4{list-style-type:none}ul.lst-kix_9e1n4ciybo9b-5{list-style-type:none}ol.lst-kix_fu03813x2h7a-3{list-style-type:none}ul.lst-kix_9e1n4ciybo9b-4{list-style-type:none}ol.lst-kix_fu03813x2h7a-6{list-style-type:none}ul.lst-kix_9e1n4ciybo9b-3{list-style-type:none}ol.lst-kix_fu03813x2h7a-5{list-style-type:none}ol.lst-kix_fu03813x2h7a-0{list-style-type:none}ul.lst-kix_9e1n4ciybo9b-8{list-style-type:none}ol.lst-kix_fu03813x2h7a-2{list-style-type:none}ul.lst-kix_9e1n4ciybo9b-7{list-style-type:none}ol.lst-kix_fu03813x2h7a-1{list-style-type:none}ul.lst-kix_14dnjwe5a6tx-7{list-style-type:none}ul.lst-kix_14dnjwe5a6tx-6{list-style-type:none}ul.lst-kix_14dnjwe5a6tx-8{list-style-type:none}ol.lst-kix_fu03813x2h7a-2.start{counter-reset:lst-ctn-kix_fu03813x2h7a-2 0}ul.lst-kix_14dnjwe5a6tx-1{list-style-type:none}ul.lst-kix_14dnjwe5a6tx-0{list-style-type:none}ul.lst-kix_14dnjwe5a6tx-3{list-style-type:none}ul.lst-kix_14dnjwe5a6tx-2{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_14dnjwe5a6tx-5{list-style-type:none}ul.lst-kix_14dnjwe5a6tx-4{list-style-type:none}.lst-kix_9e1n4ciybo9b-0>li:before{content:"\0025cf   "}.lst-kix_fu03813x2h7a-1>li{counter-increment:lst-ctn-kix_fu03813x2h7a-1}.lst-kix_fu03813x2h7a-4>li{counter-increment:lst-ctn-kix_fu03813x2h7a-4}.lst-kix_fu03813x2h7a-7>li{counter-increment:lst-ctn-kix_fu03813x2h7a-7}ol.lst-kix_fu03813x2h7a-8.start{counter-reset:lst-ctn-kix_fu03813x2h7a-8 0}ol{margin:0;padding:0}table td,table th{padding:0}.c32{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#d9d9d9;border-left-style:solid;border-bottom-width:1pt;width:175.5pt;border-top-color:#000000;border-bottom-style:solid}.c17{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#d9d9d9;border-left-style:solid;border-bottom-width:1pt;width:117.8pt;border-top-color:#000000;border-bottom-style:solid}.c16{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:225.7pt;border-top-color:#000000;border-bottom-style:solid}.c11{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:124.5pt;border-top-color:#000000;border-bottom-style:solid}.c84{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:451.4pt;border-top-color:#000000;border-bottom-style:solid}.c74{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:93.8pt;border-top-color:#000000;border-bottom-style:solid}.c63{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:158.2pt;border-top-color:#000000;border-bottom-style:solid}.c64{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:114pt;border-top-color:#000000;border-bottom-style:solid}.c15{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117pt;border-top-color:#000000;border-bottom-style:solid}.c39{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:175.5pt;border-top-color:#000000;border-bottom-style:solid}.c95{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:98.2pt;border-top-color:#000000;border-bottom-style:solid}.c42{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:95.2pt;border-top-color:#000000;border-bottom-style:solid}.c72{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:100.5pt;border-top-color:#000000;border-bottom-style:solid}.c66{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:117.8pt;border-top-color:#000000;border-bottom-style:solid}.c31{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:111.8pt;border-top-color:#000000;border-bottom-style:solid}.c6{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:107.2pt;border-top-color:#000000;border-bottom-style:solid}.c35{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:116.2pt;border-top-color:#000000;border-bottom-style:solid}.c1{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left;height:16pt}.c5{background-color:#ffffff;color:#212121;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c44{-webkit-text-decoration-skip:none;color:#434343;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c18{color:#4a86e8;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c21{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c24{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c22{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c25{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c49{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c80{color:#041834;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Open Sans"}.c55{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial"}.c75{color:#041834;font-weight:900;text-decoration:none;vertical-align:baseline;font-size:18pt;font-family:"Raleway"}.c91{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:justify;height:16pt}.c40{-webkit-text-decoration-skip:none;color:#000000;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-family:"Arial"}.c27{color:#212121;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c93{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:justify}.c36{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c47{color:#191919;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c65{-webkit-text-decoration-skip:none;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-family:"Arial"}.c12{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c23{color:#434343;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c79{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:center}.c29{background-color:#ffffff;padding-top:20pt;padding-bottom:4pt;line-height:1.15;text-align:left}.c81{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c56{padding-top:12pt;padding-bottom:12pt;line-height:1.15;text-align:justify}.c57{padding-top:3pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c103{margin-left:-54.8pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c105{margin-left:-50.2pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c87{padding-top:12pt;padding-bottom:12pt;line-height:1.15;text-align:left}.c89{color:#1b1b1b;text-decoration:none;vertical-align:baseline;font-family:"Courier New"}.c92{margin-left:-42pt;border-spacing:0;border-collapse:collapse;margin-right:auto}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:center}.c20{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:justify}.c71{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:right}.c53{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#4a86e8;text-decoration:underline}.c58{padding-top:0pt;padding-bottom:12pt;line-height:1.15;text-align:left}.c94{padding-top:12pt;padding-bottom:2pt;line-height:1.15;text-align:left}.c101{margin-left:auto;border-spacing:0;border-collapse:collapse;margin-right:auto}.c97{border-spacing:0;border-collapse:collapse;margin-right:auto}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c82{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c69{color:#000000;font-weight:700;font-size:18pt}.c8{orphans:2;widows:2;height:11pt}.c85{color:#000000;font-weight:400;font-size:20pt}.c60{text-decoration:none;vertical-align:baseline;font-family:"Arial"}.c59{text-decoration:none;vertical-align:baseline;font-family:"Roboto"}.c14{orphans:2;widows:2}.c100{font-size:18pt;font-family:"Roboto"}.c46{padding:0;margin:0}.c106{max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c73{font-weight:700;font-size:14pt}.c38{font-weight:400;font-size:11pt}.c67{font-size:12pt;font-weight:700}.c9{font-size:11pt;font-weight:700}.c99{color:#041834;font-size:11pt}.c28{margin-left:36pt;padding-left:0pt}.c30{background-color:#ffffff;color:#212121}.c52{font-weight:400;font-family:"Roboto"}.c104{color:#041834;font-size:9pt}.c107{font-weight:400;font-size:8pt}.c88{color:#000000;font-size:16pt}.c34{color:inherit;text-decoration:inherit}.c10{font-style:italic}.c26{color:#666666}.c45{color:#444746}.c50{color:#212121}.c48{color:#434343}.c68{margin-left:36pt}.c98{color:#191919}.c102{background-color:#d9d2e9}.c43{margin-left:-4pt}.c13{height:11pt}.c4{height:0pt}.c77{color:#0000ff}.c51{color:#1f1f1f}.c33{background-color:#ffffff}.c41{background-color:#d9d9d9}.c76{font-weight:700}.c78{background-color:#ffff00}.c70{height:20pt}.c90{background-color:#b6d7a8}.c83{height:47.7pt}.c19{color:#0b57d0}.c86{color:#1155cc}.c96{color:#000000}.c54{font-style:normal}.c61{color:#4a86e8}.c62{color:#980000}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c33 c106 doc-content"><div><p class="c2 c8"><span class="c0"></span></p></div><h1 class="c81 c14 c70" id="h.1cvhsvtfw6e3"><span class="c60 c54 c69"></span></h1><h1 class="c14 c79" id="h.bju35k4ans1w"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 399.64px; height: 248.03px;"><img alt="" src="images/image20.png" style="width: 399.64px; height: 248.03px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c7"><span class="c98 c76 c100">Wearable Digital Sensors for Early Detection of Mild Cognitive Impairment</span></p><h1 class="c79 c14 c70" id="h.d4jlec7zs9eu"><span class="c85 c60 c54"></span></h1><p class="c2 c8"><span class="c60 c73 c54 c96"></span></p><table class="c101"><tr class="c4"><td class="c16 c41" colspan="1" rowspan="1"><p class="c7"><span class="c60 c73 c96 c54">Name </span></p></td><td class="c16 c41" colspan="1" rowspan="1"><p class="c7"><span class="c60 c73 c96 c54">Matric Number </span></p></td></tr><tr class="c4"><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c49">Amanda Goh</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c49">A0277481U</span></p></td></tr><tr class="c4"><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c49">Kelvyna Lee En</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c49">A0257628R</span></p></td></tr><tr class="c4"><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c49">Sundhar Piradnya Yeshawini </span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c7"><span class="c49">A0266687H</span></p></td></tr></table><p class="c20 c8"><span class="c0"></span></p><p class="c7 c13"><span class="c54 c80"></span></p><hr style="page-break-before:always;display:none;"><p class="c2 c8"><span class="c40 c9 c54"></span></p><p class="c7 c14"><span class="c76 c82">Acknowledgements</span></p><p class="c2 c8"><span class="c40 c9 c54"></span></p><p class="c7 c14"><span>We would like to express our sincere gratitude and appreciation to </span><span class="c76">Dr Yen Shih-Cheng</span><span>&nbsp;and </span><span class="c76">Dr Changsheng Wu</span><span class="c0">&nbsp;for their guidance throughout the project. Their supervision has been invaluable in shaping the direction and focus of our work.</span></p><p class="c7 c8"><span class="c0"></span></p><p class="c7 c14"><span>We would also like to thank our mentor, </span><span class="c76">Mr Wang He,</span><span>&nbsp;for sharing insights from previous years of the research project. Finally, we would like to thank </span><span class="c76">Mr Yutong Du</span><span class="c0">&nbsp;for generously sharing his Python code for gait analysis and for offering his expertise in data processing.</span></p><p class="c2 c8"><span class="c40 c9 c54"></span></p><hr style="page-break-before:always;display:none;"><p class="c2 c8"><span class="c40 c9 c54"></span></p><p class="c7 c14"><span class="c3">Contributors:</span></p><p class="c2 c14"><span class="c0">This was a group project, the literature reviews, design of sensor application as well as evaluations for speech were done by Kelvyna, Amanda and Piradnya.</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14"><span class="c0">This is the team&rsquo;s individual contributions</span></p><ol class="c46 lst-kix_fu03813x2h7a-0 start" start="1"><li class="c2 c14 c28 li-bullet-0"><span class="c0">Kelvyna: Project Background, Motivation, Gait data Collection and Analysis; Prototype development; refined report language and presentation</span></li><li class="c2 c14 c28 li-bullet-0"><span class="c0">Amanda: Project scope, Gait concepts, Speech testing (backwards), Overview of gait and speech, Future project plan.Figures for introduction</span></li><li class="c2 c14 c28 li-bullet-0"><span class="c0">Piradnya &nbsp;- Gait Data collection, Prototype, Analysis &amp; Fixed feature discrepancies; Speech Data Collection Methods, Testing (Caterpillar Passage) &amp; Data Analysis</span></li></ol><p class="c2 c8 c68"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c2 c8"><span class="c40 c9 c54"></span></p><p class="c2 c8"><span class="c40 c9 c54"></span></p><p class="c2 c8"><span class="c40 c9 c54"></span></p><p class="c7 c14"><span class="c40 c9 c54">CONTENT PAGE</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c57"><span class="c3"><a class="c34" href="#h.nojg9no83qpy">1. Introduction&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.27vityb2uvhn">1.1 Context and Motivation&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.q7fkgitcsq98">1.2 Literature Review of Current Technology&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.p7pkc7yx31j2">1.3 &nbsp;Project Objectives&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.ewadokzzfq7">1.4 Project Specifications&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;8</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.nsliuptslhwp">1.5 &nbsp;Project Scope&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9</a></span></p><p class="c57"><span class="c3"><a class="c34" href="#h.z0l6ao8ts54h">2. Gait&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.2kd9bzeqgleb">2.1 Introduction to Gait Concepts&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.n2mw25ky78ny">2.2 Literature Review&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;11</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.d9mewd334gb">2.3 Data Collection &amp; Prototype&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.1dg9smwm54w6">2.4 Gait Data Analysis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14</a></span></p><p class="c57"><span class="c3"><a class="c34" href="#h.vsuye0dy77c">3. Speech&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.97lpo4jdwq71">3.1 Literature Review&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.7d0gvrxn9ghc">3.2 Data Collection Methods&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;22</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.535tcq8vlbn6">3.3 Speech Testing - Caterpillar Text&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;24</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.1qgan1g3o6u9">3.4 Speech Data Analysis&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;25</a></span></p><p class="c12"><span class="c0"><a class="c34" href="#h.jv0ndesodpcq">3.5 Evaluation&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;28</a></span></p><p class="c57"><span class="c3"><a class="c34" href="#h.yse2nxxdy54p">4. Summary&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;29</a></span></p><p class="c57"><span class="c3"><a class="c34" href="#h.tbolmsxh67hs">5. Future Project Plan&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;30</a></span></p><p class="c57"><span class="c3"><a class="c34" href="#h.wl0d9y6j6atq">6. References&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;31</a></span></p><h1 class="c81 c14 c70" id="h.2jfk0ca2atma"><span class="c40 c67 c54"></span></h1><h1 class="c81 c14 c70" id="h.cg7mzeug78hf"><span class="c40 c67 c54"></span></h1><p class="c2 c8"><span class="c0"></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c8"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c2 c8"><span class="c0"></span></p><h1 class="c81 c14" id="h.nojg9no83qpy"><span class="c67">1. Introduction</span></h1><h2 class="c36 c14" id="h.27vityb2uvhn"><span class="c9">1.1 Context and Motivation</span></h2><p class="c20 c14"><span>Mild Cognitive Impairment (MCI) is widely recognised as an intermediate stage between normal age-related cognitive decline and dementia. (</span><span>Eshkoor et al.,2015)</span><span>. It is characterised by noticeable deficits in memory, attention, and judgment that are greater than expected for a person&rsquo;s age, yet not severe enough to significantly interfere with daily activities (</span><span>Cipriani et al.2020)</span><span class="c0">. Individuals with MCI can carry out daily activities, but may experience increasing difficulty in performing complex tasks.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>MCI is a critical focus area for research because it represents a potential window for early intervention before irreversible neurodegeneration occurs. While some individuals may recover normal cognitive function, numerous longitudinal studies have shown that people with MCI are at a significantly higher risk of progressing to dementia or Alzheimer&rsquo;s disease (</span><span>Petersen et al., 2018; Jongsiriyanyong &amp; Limpawattana, 2018</span><span class="c0">).</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>In Singapore, approximately 12.5% of adults above the age of 60 are estimated to have MCI. </span><span>(Subramaniam et al., 2015)</span><span>. With the country&rsquo;s rapidly ageing population, this prevalence and its associated healthcare burden are expected to increase significantly. The Ministry of Health projects that up to 152,000 people could be living with dementia by 2030, incurring an additional S$106 million in annual healthcare expenditure. Findings have shown that individuals with cognitive impairment incur an additional $700 of healthcare costs annually, largely due to increased emergency care visits and hospitalisation. </span><span>(Duke-NUS, 2024) Another study has shown that t</span><span>he median annual cost of care for cognitively impaired patients is approximately S$13,800, driven largely by repeated medical consultations, imaging, and long-term management. </span><span>(Woo et al., 2017)</span></p><hr style="page-break-before:always;display:none;"><h2 class="c1" id="h.8fq76mm3l3l5"><span class="c40 c9 c54"></span></h2><h2 class="c36 c14" id="h.q7fkgitcsq98"><span class="c9">1.2 Literature Review of Current Technology </span></h2><p class="c2 c8"><span class="c40 c9 c10"></span></p><table class="c103"><tr class="c83"><td class="c95 c41" colspan="1" rowspan="1"><p class="c7 c13"><span class="c25"></span></p></td><td class="c64 c41" colspan="1" rowspan="1"><p class="c7"><span class="c25">Context</span></p></td><td class="c6 c41" colspan="1" rowspan="1"><p class="c7"><span class="c25">Key features of device screening for MCI</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c7"><span class="c25">Findings </span></p></td><td class="c31 c41" colspan="1" rowspan="1"><p class="c7"><span class="c25">Relevance to Project</span></p></td></tr><tr class="c4"><td class="c95" colspan="1" rowspan="1"><p class="c2"><span class="c21">Kang et al., 2022</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">Investigated patients with Parkinson&rsquo;s Disease to determine whether gait disturbance is associated with mild cognitive impairment (MCI) and which cognitive functions most affect gait changes.</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c21">Used wearable inertial measurement units (IMUs) to record spatiotemporal gait parameters during walking tasks. <br><br>Device measures acceleration and angular velocity to compute cadence, stride time, and gait variability. </span></p></td><td class="c66" colspan="1" rowspan="1"><p class="c2"><span class="c21">Individuals with MCI have slower gait speed, reduced cadence, increased stride time variability, and require longer double support times. </span></p><p class="c2 c13"><span class="c21"></span></p><p class="c2"><span class="c21">These parameters are correlated with poorer executive and memory function.</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c21">Supports the use of accelerometer-based gait analysis for early detection of mild cognitive decline. </span></p><p class="c2 c13"><span class="c21"></span></p><p class="c2"><span class="c21">Subtle changes in gait can serve as non&ndash;invasive biomarkers for MCI. As such, this project can also use gait features derived from IMU data to screen for MCI.</span></p></td></tr><tr class="c4"><td class="c95" colspan="1" rowspan="1"><p class="c2"><span class="c21">(Li et al., 2023)</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">Adults above 65 years old were assessed using community screening tools for MCI using digital cognitive tests and wearable devices.</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c21">Tablet-based digital cognitive tasks were assigned to participants.</span></p><p class="c2 c13"><span class="c21"></span></p><p class="c2"><span class="c21">Participants had to wear physiological sensors (PPG, EDA, EEG) during resting state and task state.</span></p></td><td class="c66" colspan="1" rowspan="1"><p class="c2"><span class="c21">The MCI group took a longer time to complete cognitive tasks, showed a lower heart rate variation, and higher electrodermal activity.</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c21">Supports focus on non-invasive sensors and signal processing for detection of early MCI.</span></p><p class="c2 c13"><span class="c21"></span></p><p class="c2"><span class="c21">Demonstrates the value in combining digital cognitive tests and wearable sensors to screen for MCI.</span></p></td></tr><tr class="c4"><td class="c95" colspan="1" rowspan="1"><p class="c2"><span class="c21">Dodge et al., 2022</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">Explored smart-home sensor data to identify daily activity and walking speed as predictors of cognitive decline </span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c21">Uses passive motion sensors embedded in home environments. It uses longitudinal tracking of movement patterns for participants and cloud-based analytics with machine learning models </span></p></td><td class="c66" colspan="1" rowspan="1"><p class="c2"><span class="c21">Decline in walking speed and increased variability predicted future MCI conversion within 2 years</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c21">Highlights the predictive value of gait parameters and informs how continuous, unobstructive motion tracking and non-invasive sensor could be used for preventive screening of MCI. </span></p></td></tr></table><p class="c7 c14"><span class="c37 c10">Table 1: Summary of Current Technology in relation to MCI detection</span></p><p class="c20 c8"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">The growing prevalence of MCI and dementia highlights the urgent need for early, accessible, and cost-effective detection methods. Table 1 summarises current technology available in relation to the detection of MCI. </span></p><p class="c20 c8"><span class="c0"></span></p><p class="c14 c20"><span>Current screening and diagnostic procedures include Magnetic Resonance Imaging (MRI) or Positron Emission Tomography (PET) brain scans, neuropsychological testing, and biochemical analysis of cerebrospinal fluid. While these are definitive and accurate, they are expensive, invasive, and resource-intensive. These hinder widespread adoption of screening for MCI, especially for early or community-based screening. Additionally, such assessments are typically performed only after cognitive symptoms become clinically apparent. Early diagnosis remains a challenge, as subtle physiological and behavioural changes frequently go unnoticed in routine clinical visits. (</span><span>Wenier et al.,2023)</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>Given these challenges, there is a need to develop a non-invasive, scalable, and affordable tool that can continuously monitor subtle indicators of cognitive decline. </span></p><p class="c20 c8"><span class="c3"></span></p><hr style="page-break-before:always;display:none;"><h2 class="c91 c14" id="h.6vlo5ss436tn"><span class="c3"></span></h2><h2 class="c14 c93" id="h.p7pkc7yx31j2"><span class="c9">1.3 &nbsp;Project Objectives </span></h2><p class="c20 c14"><span>This</span><span>&nbsp;project is conducted in collaboration with Professor Wu Changsheng&rsquo;s laboratory, </span><span>which </span><span>partners with clinicians and students from the Saw Swee Hock School Of Public Health. It forms part of a larger ongoing research program aimed at developing a non-invasive wearable mechano-acoustic sensor system to detect gait and speech-based biomarkers of MCI. Ultimately, the broader goal is to enable community-wide and non-invasive screening for early cognitive decline, supporting earlier intervention and reducing the long-term healthcare burden. As such, the overarching research problem statement is: &ldquo;</span><span class="c10 c76">How might we make the detection of Mild Cognitive Impairment more timely, affordable, and non-invasive</span><span class="c76">?</span><span class="c0">&rdquo;. Figure 1 presents the patient journey, providing an overview of how the device is intended to be used in community screenings.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c8"><span class="c40 c9 c54"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 539.85px; height: 371.94px;"><img alt="" src="images/image13.png" style="width: 539.85px; height: 371.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c10">Figure 1: Summary of Patient Journey</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">As part of the research program, this project aims to determine whether gait and speech features measured through wearable accelerometers can serve as reliable indicators of MCI, forming the analytical foundation for future scalable screening programmes in Singapore.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">Through exploring how measurable differences in movement and vocal vibration patterns point to MCI, this project aims to address the following problem statements:</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c60 c9 c10 c96">&ldquo;How might we use accelerometer signals collected from the Inertial Measurement Unit (IMU) to identify gait parameters associated with MCI?&rdquo;</span></p><p class="c20 c8"><span class="c60 c9 c10 c96"></span></p><p class="c20 c14"><span class="c10 c76">&ldquo;How might we analyse recorded speech vibration signals from accelerometers to extract features that distinguish individuals with MCI from healthy individuals?&quot;</span></p><p class="c20 c8"><span class="c0"></span></p><h2 class="c36 c14" id="h.ewadokzzfq7"><span class="c9">1.4 Project Specifications</span></h2><p class="c2 c14"><span class="c0">Table 2 summarises the overall research programme specifications.</span></p><p class="c2 c8"><span class="c0"></span></p><table class="c97"><tr class="c4"><td class="c35 c41" colspan="1" rowspan="1"><p class="c7"><span class="c3">Objectives</span></p></td><td class="c63 c41" colspan="1" rowspan="1"><p class="c7"><span class="c3">Requirements</span></p></td><td class="c32" colspan="1" rowspan="1"><p class="c7"><span class="c3">Specifications</span></p></td></tr><tr class="c4"><td class="c35" colspan="1" rowspan="1"><p class="c2"><span class="c0">Affordable detection of MCI</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c2"><span class="c0">MCI screening to happen outside clinics</span></p><p class="c2 c13"><span class="c0"></span></p></td><td class="c39" colspan="1" rowspan="1"><ul class="c46 lst-kix_x308hham3rf2-0 start"><li class="c2 c28 li-bullet-0"><span class="c0">Detect MCI from daily habits </span></li><li class="c2 c28 li-bullet-0"><span class="c0">Recording and processing of MCI feature data</span></li></ul></td></tr><tr class="c4"><td class="c35" colspan="1" rowspan="1"><p class="c2"><span class="c0">Detection of MCI from daily habits</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c2"><span class="c0">Continuous data collection</span></p></td><td class="c39" colspan="1" rowspan="1"><ul class="c46 lst-kix_9e1n4ciybo9b-0 start"><li class="c2 c28 li-bullet-0"><span class="c0">Does not obstruct daily activities</span></li><li class="c2 c28 li-bullet-0"><span class="c0">Sufficient storage for data</span></li><li class="c2 c28 li-bullet-0"><span class="c0">Long battery life, rechargeable</span></li></ul></td></tr><tr class="c4"><td class="c35" colspan="1" rowspan="1"><p class="c2"><span class="c0">User-friendly for older adults</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c2"><span class="c0">Easy to wear and operate</span></p></td><td class="c39" colspan="1" rowspan="1"><ul class="c46 lst-kix_a4m4v1rfc2mh-0 start"><li class="c2 c28 li-bullet-0"><span class="c0">Comfortable</span></li><li class="c2 c28 li-bullet-0"><span class="c0">Waterproof</span></li><li class="c2 c28 li-bullet-0"><span class="c0">Unobstructive form</span></li></ul></td></tr><tr class="c4"><td class="c35" colspan="1" rowspan="1"><p class="c2"><span class="c0">Large-scale screening tool</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c2"><span class="c0">Non-invasive data collection</span></p></td><td class="c39" colspan="1" rowspan="1"><ul class="c46 lst-kix_14dnjwe5a6tx-0 start"><li class="c2 c28 li-bullet-0"><span class="c0">Passive data collection through sensors</span></li></ul></td></tr><tr class="c4"><td class="c35" colspan="1" rowspan="1"><p class="c2"><span class="c0">Maintain patient&rsquo;s privacy</span></p></td><td class="c63" colspan="1" rowspan="1"><p class="c2"><span class="c0">Ensure data privacy throughout collection and storage</span></p></td><td class="c39" colspan="1" rowspan="1"><ul class="c46 lst-kix_sef5n0d1qmsi-0 start"><li class="c2 c28 li-bullet-0"><span class="c0">No raw audio collected (vibration signals used instead)</span></li></ul></td></tr></table><p class="c7 c14"><span class="c10">Table 2. Overall Research Project Specifications</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">The gait project specification is to use accelerometer data from the wearable sensor to extract relevant motion parameters. This aligns with the current capabilities of the research prototype developed in Professor Wu&rsquo;s lab, which only records 3-axis acceleration (x, y, z).</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">The speech component aims to extract features from non-invasive, mechano-acoustic sensors that detect vocal vibration signals instead of raw audio. This approach ensures both privacy and user comfort, especially for elderly participants. Key specifications include:</span></p><ul class="c46 lst-kix_lhqp2gpz0fi4-0 start"><li class="c20 c14 c28 li-bullet-0"><span class="c0">Sensor modality: Mechano-acoustic vibration sensor to be placed near the vocal tract</span></li><li class="c20 c14 c28 li-bullet-0"><span class="c0">Data type: Vibration-based signal (no audio recording)</span></li><li class="c20 c14 c28 li-bullet-0"><span class="c0">Measured features: Speech rate, articulation pause duration, frequency modulation, and signal amplitude patterns</span></li><li class="c20 c14 c28 li-bullet-0"><span class="c0">Privacy consideration: No linguistic content or voice data stored, only vibration signals are processed</span></li><li class="c20 c14 c28 li-bullet-0"><span class="c0">Design rationale: Enables continuous, privacy-preserving speech monitoring suitable for large-scale community deployment</span></li></ul><p class="c20 c8"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><h2 class="c1" id="h.i9zbsaotsvy0"><span class="c40 c9 c54"></span></h2><h2 class="c36 c14" id="h.nsliuptslhwp"><span class="c9">1.5 &nbsp;Project Scope</span></h2><p class="c20 c14"><span>The wider research scope includes the sensor design, software for data processing, and clinical application aspects of the wearable mechanical-acoustic sensing systems for cognitive and physiological monitoring.</span></p><p class="c20 c8"><span class="c60 c38 c51 c33 c54"></span></p><p class="c56 c14"><span>As such, </span><span>Professor Wu&rsquo;s laboratory developed a mechano-acoustic wearable sensor (&ldquo;Research Sensor&rdquo;) to capture vibration and motion signals from the throat and lower limb. This sensor consists of a battery-operated three-axis accelerometer encapsulated with silicone.</span></p><p class="c20 c8"><span class="c60 c38 c51 c33 c54"></span></p><p class="c20 c14"><span class="c51 c33">On the other hand, the group-specific focus is to conduct analysis on gait and speech data collected from the research sensor. </span><span class="c51 c33">W</span><span class="c33 c51">e are to extract and analyse the data collected to validate gait features that differ between healthy and cognitively impaired individuals. We are also to explore the feasibility of using vibration-based speech signals to identify vocal markers of MCI. </span></p><p class="c56 c14"><span class="c0">Figure 2 summarises the scope of tasks among various groups in the research team.</span></p><p class="c2 c8"><span class="c0 c90"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 582.50px; height: 348.34px;"><img alt="" src="images/image4.png" style="width: 582.50px; height: 354.66px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c10 c50">Figure </span><span class="c10 c50">2: </span><span class="c10 c50">Task Distribution Among Research Team </span></p><p class="c56 c14"><span>Due to limitations in the quantity of the research sensor, the group has decided to use a commercial sensor (&ldquo;MetaBase sensor&rdquo;) (Figure 3) for preliminary data collection and testing.</span></p><p class="c56 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 445.35px; height: 286.30px;"><img alt="" src="images/image3.png" style="width: 477.05px; height: 294.23px; margin-left: -31.70px; margin-top: -7.93px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c30 c10">Figure 3: Devices Used</span></p><h1 class="c81 c14 c70" id="h.8lqrfyouynqi"><span class="c40 c9 c54"></span></h1><hr style="page-break-before:always;display:none;"><h1 class="c14 c70 c81" id="h.ao7oqebmck3"><span class="c40 c9 c54"></span></h1><h1 class="c81 c14" id="h.z0l6ao8ts54h"><span class="c3">2. Gait </span></h1><h2 class="c36 c14" id="h.2kd9bzeqgleb"><span class="c3">2.1 Introduction to Gait Concepts </span></h2><p class="c20 c14"><span>Gait refers to one&rsquo;s walking patterns and has distinct features. Gait analysis involves measuring distinct features such as step time, stride time, distance, and stability.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>The accelerometer measures changes in acceleration as one walks. When the accelerometer data is processed, it </span><span>produces distinct and repeatable patterns shown in graphs. Different points on the graph correspond to various stages of a gait cycle. Through further processing of the time spent at different stages, differences in features, which are indicative of MCI, such as &ldquo;double support time&rdquo;, can be extracted. (Mayagoitia et al., 2002) (Prisco et.al, 2025)</span><span>. </span></p><p class="c20 c13"><span class="c0"></span></p><p class="c20"><span class="c30">E</span><span class="c30">ach gait cycle exhibits consistent characteristic patterns in accelerometer and derived angular velocity signals. </span><span class="c30">These patterns correspond to a specific gait event which serves as reference signatures for event detection. </span><span>Figure 4 &nbsp;illustrates the major phases in a gait cycle, consisting of the stance and swing phases, which account for 60% and 50% of the cycle, respectively. </span><span class="c0">Our analysis focuses on the characteristic gait events that occur within each phase. </span></p><p class="c20 c13"><span class="c0"></span></p><p class="c2 c13"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 526.65px; height: 370.33px;"><img alt="" src="images/image2.png" style="width: 526.65px; height: 416.77px; margin-left: 0.00px; margin-top: -46.44px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c30 c10">Figure 4: Phases of Gait</span></p><p class="c2 c8"><span class="c0"></span></p><h2 class="c36 c14" id="h.n2mw25ky78ny"><span class="c9">2.2 Literature Review </span></h2><p class="c20 c14"><span class="c0">To understand how the gait parameters serve as early biomarkers for MCI, several studies were reviewed. </span></p><p class="c20 c8"><span class="c0"></span></p><table class="c92"><tr class="c4"><td class="c41 c42" colspan="1" rowspan="1"><p class="c7 c13"><span class="c3"></span></p></td><td class="c11 c41" colspan="1" rowspan="1"><p class="c7"><span class="c3">Context</span></p></td><td class="c15 c41" colspan="1" rowspan="1"><p class="c7"><span class="c3">Key Gait Features Examined </span></p></td><td class="c41 c72" colspan="1" rowspan="1"><p class="c7"><span class="c3">Findings </span></p></td><td class="c64 c41" colspan="1" rowspan="1"><p class="c7"><span class="c3">Relevance to Project</span></p></td></tr><tr class="c4"><td class="c42" colspan="1" rowspan="1"><p class="c2"><span class="c21">Montero-Odasso et al., 2012</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c2"><span class="c21">Investigate the impact of cognitive load on gait among older adults using single and dual-task walking </span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c2"><span class="c21">Gait speed, stride time, variability, dual-task cost</span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c2"><span class="c21">The MCI group showed slower gait and high variability, especially during dual-task walking </span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">This demonstrates that the gait performance under cognitive load can reveal early cognitive decline. This supports the inclusion of dual-task protocols in our sensor testing </span></p></td></tr><tr class="c4"><td class="c42" colspan="1" rowspan="1"><p class="c2"><span class="c21">Prasanth et al., 2021</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c2"><span class="c21">Validated IMU-based gait analysis against Vicon motion capture in healthy adults</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c2"><span class="c21">Stride time, step length, cadence, and &nbsp;double support duration</span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c2"><span class="c21">IMU-derived gait features correlated strongly with the Vicon </span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">This supports the reliability of wearable IMUs for accurate gait features extraction, justifying our use of MetaBase sensors for data collection</span></p></td></tr><tr class="c4"><td class="c42" colspan="1" rowspan="1"><p class="c2"><span class="c21">Beauchet et al., 2014</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c2"><span class="c21">Compared gait parameters between MCi subtypes and healthy controls in community-dwelling older adults</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c2"><span class="c21">Velocity, stride length, variability, and &nbsp;stride time </span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c2"><span class="c21">Individuals with MCI, partially amnestic type, had reduced velocity and greater variability</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">This highlights specific gait parameters (stride time, variability, velocity) as potential biomarkers for MCI classification</span></p></td></tr><tr class="c4"><td class="c42" colspan="1" rowspan="1"><p class="c2"><span class="c21">He et al., 2024</span></p></td><td class="c11" colspan="1" rowspan="1"><p class="c2"><span class="c21">Examined the accuracy of IMU-based gait analysis in healthy females for clinical application</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c2"><span class="c21">Step time, cadence, swing phase, stride length </span></p></td><td class="c72" colspan="1" rowspan="1"><p class="c2"><span class="c21">Reported string agreement with gold standard systems </span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">This validates wearable gait sensing methods for clinical -grade sensing methods for clinical-grade data collection, reinforcing the feasibility of low-cost, mobile screening.</span></p></td></tr></table><p class="c7 c14"><span class="c10">Table 3. Summary of Gait-related Studies on MCI Detection</span><span class="c40 c9 c10">&nbsp;</span></p><p class="c7 c8"><span class="c40 c9 c10"></span></p><p class="c20 c14"><span class="c0">Gait is increasingly recognised as a sensitive indicator of cognitive health because it depends on complex brain networks that control attention, executive function, and motor coordination. In individuals with MCI, disruptions in these can lead to subtle measurable changes in walking patterns. Studies consistently report slower gait speed, greater stride time variability, and longer double support duration among MCI groups compared to healthy controls. Collectively, these studies suggest that gait reflects both motor and cognitive integrity. Since such changes often emerge before noticeable memory decline, gait analysis offers a promising non-invasive MCI screening tool to detect early cognitive impairment.</span></p><p class="c20 c8"><span class="c0"></span></p><h2 class="c93 c14" id="h.d9mewd334gb"><span class="c9">2.3 Data Collection &amp; Prototype</span></h2><p class="c20 c14"><span class="c0">Since the research sensor only uses an accelerometer for data collection, only the accelerometer was activated in the MetaBase sensor. The MetaBase sensor was placed on the thigh and set to a 100 Hz sampling rate to recreate previous gait data collection protocol. Accelerometer data was successfully collected and analysed by converting raw 3 axis data to thigh gait angles.</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">In the established thigh gait cycle as shown in Figure 5, gait phases are distinguished within a very close margin. This is because device placement is further away from the main joints (knee, ankle, hip) commonly used for gait analysis. In the case of MCI patients, this threshold is reduced even more due to irregular gait, causing phases to be similar to each other, increasing the chance of unclear event detection.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 616.50px; height: 171.87px;"><img alt="" src="images/image19.png" style="width: 616.50px; height: 171.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span class="c10">Figure 5: Thigh Angle Gait Plot </span></p><p class="c7 c14"><span class="c37 c10">Left: Test data, Right: Established Literature</span></p><p class="c7 c14"><span class="c37 c10">(Abhayasinghe &amp; Murray, 2014)</span></p><p class="c7 c8"><span class="c37 c10"></span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>Due to these limitations, we experimented to place the sensor at a different location, the ankle (lower lateral shank), to collect gait data (Panebianco et al., 2020; Vargas-Valencia et al., 2016; Trojaniello et al., 2014). </span><span class="c0">Testing encompassed securing the MetaBase sensor to the ankle and performing daily activities (walking in a straight line, up/down slope and up/down stairs). From the ankle data collected, significant gait features were identifiable from raw and minimally processed data (Figure 6). Additionally, ankle gait was distinct, cyclic and resilient to daily activities. </span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 158.67px;"><img alt="" src="images/image7.png" style="width: 601.70px; height: 158.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c14"><span class="c10">Figure 6: Raw Accelerometer Data (x, y, z, magnitude)</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>Furthermore, literature shows that raw accelerometer gait (1-axis) can be used to differentiate between normal and altered walk; and a conversion to frequency domain can enable the quantification of cadence associated with normal and altered walk, as shown in Figure 7 </span><span class="c10">(Canonico et al., 2023b).</span><span>&nbsp;This substantiates the idea that ankle data is more significant for our purpose of MCI gait detection from daily movement.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c8"><span class="c37 c10"></span></p><p class="c7 c8"><span class="c37 c10"></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 365.33px;"><img alt="" src="images/image8.png" style="width: 601.70px; height: 365.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c10">Figure 7: Walking characteristics from raw and FFT accelerometer data </span></p><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">Previous trials performed by the research team revealed insights into difficulties with adherence of gait sensors to the thigh. We have ideated and improved the mode of securing the gait sensor using velcro-based straps to house the sensor, shown in Figure 8. This provides a secure attachment of the sensor to the ankle and is an easier method of device attachment for MCI patients.</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 529.00px; height: 211.00px;"><img alt="" src="images/image23.png" style="width: 529.00px; height: 211.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c37 c10">Figure 8: Velcro Strap Band Housing Sensor</span></p><p class="c2 c8"><span class="c37 c10"></span></p><hr style="page-break-before:always;display:none;"><h2 class="c14 c36" id="h.1dg9smwm54w6"><span class="c9">2.4 Gait Data Analysis</span></h2><p class="c20 c14"><span class="c0">As previously mentioned, certain gait features differ significantly between individuals with normal cognition and those with MCI. Because our primary objective is to extract these clinically relevant gait indicators, accurate step and event detection is essential. Reliable feature extraction allows for the precise calculation of gait parameters that may reflect early cognitive decline. </span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>The group adopted a previously developed </span><span>feature </span><span class="c0">detection code (hereinafter referred to as &ldquo;legacy code&rdquo;). This code was intended to detect gait events across multiple cycles by locating characteristic peaks and troughs in the processed signal. However, when applied to data collected using the current MetaBase sensor, several detection errors arose due to differences in signal characteristics and timing patterns between sensors.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">To ensure event detection is accurate, it is important to resolve these discrepancies by modifying the legacy code so that gait events are consistently and correctly identified for data collected by the MetaBase sensor. Once event detection is corrected, we can extract key gait features and validate their accuracy against Vicon, the industry gold standard for motion capture. </span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>Vicon is an industry-standard motion capture system that uses infrared cameras to detect reflective markers placed on participants. The Vicon software then processes the gait movement of participants with respect to time and space, allowing for identification of gait events and the quantification of specific gait parameters (Vicon Motion Systems, 2025) such as stride time, step time, and double support time.</span></p><p class="c20 c8"><span class="c0 c78"></span></p><p class="c20"><span class="c82 c33 c76">Discrepancy #1: Feature Detection Within Gait Data Sets </span></p><p class="c7 c8"><span class="c0"></span></p><p class="c20 c13"><span class="c5"></span></p><p class="c20 c13"><span class="c5"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 317.33px;"><img alt="" src="images/image25.png" style="width: 601.70px; height: 317.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><table class="c97"><tr class="c4"><td class="c84" colspan="1" rowspan="1"><p class="c7"><span class="c10 c99">Figure 9: Identifiable Gait Features in a Gait Cycle </span></p></td></tr></table><p class="c2 c13"><span class="c5"></span></p><p class="c20"><span class="c30">Figure 9 shows expected angular velocity profiles associated with these events. </span><span class="c30 c10">Toe Off</span><span class="c30">&nbsp;is marked by rapid rotation of the ankle, producing a negative peak. </span><span class="c10 c30">Pre-Swing</span><span class="c30">&nbsp;involves controlled flexion of the foot, resulting in a slight positive angular velocity. </span><span class="c30 c10">Mid Swing</span><span class="c30">&nbsp;and </span><span class="c30 c10">Heel Strike</span><span class="c30">&nbsp;occur when the ankle is near a neutral position, resulting in minimal angular movement. Meanwhile, </span><span class="c30 c10">Terminal Stance </span><span class="c30">and </span><span class="c30 c10">Terminal Swing</span><span class="c30">&nbsp;generate distinct positive velocity peaks of similar magnitude, reflecting maximum ankle flexion during these phases</span></p><p class="c2 c13"><span class="c5"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 193.33px;"><img alt="" src="images/image17.png" style="width: 601.70px; height: 193.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c27 c10 c33">Figure 10: Set of gait data collected from the right ankle of a healthy participant. Gait Event Detection by Unmodified Code</span></p><p class="c2 c13"><span class="c5"></span></p><p class="c20"><span class="c50">It is important to ensure that the algorithm distinguishes these seven events from the processed data. As seen in Figure 10, </span><span class="c98">in the second cycle</span><span class="c98">, it marked the terminal swing at the wrong point, and on the fourth cycle, it missed the terminal swing, toe-off, and heel strike</span><span class="c98">.</span></p><p class="c20 c13"><span class="c47 c38"></span></p><p class="c20"><span class="c38 c47">Looking into the code flow in further detail, a potential root cause was identified.</span></p><p class="c2 c13"><span class="c27 c54"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 525.35px; height: 264.58px;"><img alt="" src="images/image15.png" style="width: 525.35px; height: 264.58px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c10 c27">Figure 11: Legacy Code Flow. </span></p><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">The data processing to identify significant gait phases is dependent on the Terminal swing onset identified within each gait cycle. This identification is performed by a peak finding function. </span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">Further analysis (Figure 12) shows that the terminal swing and terminal stance peaks in the 2nd gait cycle are significantly close to each other. Hence, the peak finding function ignores the 2nd peaks, causing misalignment of gait features in the following gait cycles.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 352.85px; height: 267.62px;"><img alt="" src="images/image21.png" style="width: 357.00px; height: 267.62px; margin-left: -4.15px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c27 c10">Figure 12 : Right Shank Angle Peaks. Close peaks discarded by peak finding algorithm</span></p><p class="c20 c13"><span class="c0"></span></p><p class="c20"><span class="c50">Further analysis of the code showed that the minimum separation coefficient distinguishes and decides the minimum duration between the same events over different cycles. </span><span class="c50">To improve event detectio</span><span class="c50">n, this coefficient</span><span class="c50">&nbsp;in the code was modified from 0.75 to 0.2. By reducing the minimum separation coefficient, the new code is able to accurately detect all missing features, such as </span><span class="c10 c50">Right Heel Strike</span><span class="c50">, </span><span class="c10 c50">Toe Off,</span><span class="c50">&nbsp;and </span><span class="c10 c50">Terminal Swing Onset </span><span class="c50">(Figure 13)</span><span class="c27 c54">. </span></p><p class="c20 c13"><span class="c27 c54"></span></p><p class="c20 c13"><span class="c27 c54"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 186.67px;"><img alt="" src="images/image5.png" style="width: 601.70px; height: 186.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c27 c10">Figure 13 : Gait Event Detection (Modified Code). </span></p><p class="c7"><span class="c27 c10">All features of gait identified in each gait cycle for the whole dataset</span></p><p class="c20 c13"><span class="c27 c54"></span></p><hr style="page-break-before:always;display:none;"><p class="c20 c13"><span class="c65 c9 c50 c54"></span></p><p class="c20"><span class="c82 c50 c76">Discrepancy #2: Differences Within Features Identified by Vicon and Code</span></p><p class="c20"><span class="c27 c54">The code uses the identified gait features (as seen in Figure 9) to generate a heel strike and toe off pair for each gait cycle in the data set. Using the angular velocity and time associated with each gait feature in the pair, quantitative features are processed.</span></p><p class="c20 c13"><span class="c27 c54"></span></p><p class="c20"><span class="c50">Processed f</span><span class="c50">eatures such as cadence, step time, and stride length calculated by the legacy code closely match Vicon processed data with an average 5% error throughout all participants. However, features such as foot off and opposite foot off present high percentages of error (&gt;100%), as seen in </span><span class="c50">Figure 14.</span><span class="c27 c54">&nbsp;Hence, the primary aim is to identify strategies to reduce this error. The future aim would be to achieve accuracy close to Vicon-processed data. </span></p><p class="c2 c13"><span class="c0"></span></p><p class="c20"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 446.67px;"><img alt="" src="images/image16.png" style="width: 601.70px; height: 446.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c27 c10 c33">Figure 14: Percentage (%) feature error comparison for Foot off and Opp Foot off</span></p><p class="c20 c13"><span class="c27 c10 c33"></span></p><p class="c20"><span class="c30 c10">Foot off</span><span class="c30">&nbsp;indicates the percentage of the gait cycle during which a foot is lifted off the ground. </span><span class="c30 c10">Opposite foot off</span><span class="c5">&nbsp;indicates the percentage for which the opposite foot is off the ground with respect to the current leg gait cycle (Vicon Motion Systems, 2019). </span></p><p class="c20 c13"><span class="c5"></span></p><p class="c20"><span class="c30">Comparing the processed data results output by the legacy code and Vicon, </span><span class="c30 c10">foot off</span><span class="c30">&nbsp;(%) was consistently lower, and </span><span class="c30 c10">opposite foot off (%)</span><span class="c5">&nbsp;was consistently higher.</span></p><p class="c20 c13"><span class="c5"></span></p><p class="c20"><span class="c5">This ratio was constant, enabling the identification that the foot off needs to be increased to approximately 280% of the value, and the opposite foot off needs to be decreased to ~35% of the value. The ratio difference can be potentially attributed to changes due to different versions of the commercial Metabase sensor.</span></p><p class="c20 c13"><span class="c5"></span></p><p class="c20"><span class="c30">This modification of the ratio in gait feature calculation reduced errors with each feature value </span><span class="c30 c10">(Foot off </span><span class="c30">and</span><span class="c30 c10">&nbsp;Opp Foot off) </span><span class="c5">across data sets, averaging less than 50% error in comparison to Vicon data.</span></p><hr style="page-break-before:always;display:none;"><h1 class="c29 c14 c70" id="h.n6fyx0jj0o51"><span class="c40 c67 c54"></span></h1><h1 class="c29 c14" id="h.vsuye0dy77c"><span class="c24">3. Speech </span></h1><h2 class="c36 c14" id="h.97lpo4jdwq71"><span class="c3">3.1 Literature Review</span></h2><p class="c20 c14"><span>Speech is closely linked to cognitive processes such as memory, attention, and language, making it a sensitive indicator of early cognitive decline. In MCI, subtle changes can be observed, for example, people tend to speak more slowly, pause more often, and show reduced fluency. Their language also becomes simpler, with a more limited vocabulary and less coherent expression. Acoustic features such as pause duration, speech rate, jitter, and phonation stability can reliably distinguish MCI from healthy aging, reflecting the brain&rsquo;s reduced efficiency in language planning and retrieval. As a result, speech analysis is a promising non-invasive tool for early MCI detection. &nbsp;A summary of the literature findings are reflected in </span><span>Table 4.</span></p><p class="c2 c8"><span class="c40 c9 c10"></span></p><table class="c105"><tr class="c83"><td class="c74 c41" colspan="1" rowspan="1"><p class="c7 c13"><span class="c25"></span></p></td><td class="c64 c41" colspan="1" rowspan="1"><p class="c7"><span class="c25">Context</span></p></td><td class="c6 c41" colspan="1" rowspan="1"><p class="c7"><span class="c25">Key Speech Features Examined</span></p></td><td class="c17" colspan="1" rowspan="1"><p class="c7"><span class="c25">Findings</span></p></td><td class="c31 c41" colspan="1" rowspan="1"><p class="c7"><span class="c25">Relevance to Project </span></p></td></tr><tr class="c4"><td class="c74" colspan="1" rowspan="1"><p class="c2"><span class="c21">(van den Berg et al. (2024)</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c104">Evaluated the feasibility, reliability, and amyloid-&beta; (A&beta;) associations of remote, multi-day tablet-based speech assessments in cognitively unimpaired older adults (preclinical AD stage)</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c21">Temporal and acoustic features such as pause-to-word ratio, pause duration, phonation rate, fundamental frequency, intensity variance, jitter, shimmer</span></p></td><td class="c66" colspan="1" rowspan="1"><p class="c2"><span class="c21">A&beta;-positive (preclinical) individuals showed more pauses (higher pause-to-word ratio) and lower phonation rate compared to A&beta;-negative; trends similar to early MCI speech decline. Multi-day averaging improved test-retest reliability (ICC &ge; 0.75)</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c21">Supports using pausing behavior and phonation rate as early digital biomarkers for cognitive decline, justifies our focus on speech timing metrics from throat accelerometer data, and a multi-day, at-home data collection approach for consistent screening.</span></p></td></tr><tr class="c4"><td class="c74" colspan="1" rowspan="1"><p class="c2"><span class="c21">(Federation University, 2025, May 30)</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">AI-based speech analysis tool for early Alzheimer&rsquo;s detection using multimodal input &mdash; both linguistic (text) and acoustic (sound) features. </span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c21">Linguistic features: Vocabulary richness, sentence complexity, word choice, and semantic coherence. Acoustic features: Pitch, rhythm, pauses,and fluency patterns </span></p></td><td class="c66" colspan="1" rowspan="1"><p class="c2"><span class="c21">People with Alzheimer s use simpler vocabulary, fewer details, and show pauses or irregular prosody compared to healthy controls. </span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c21">Supports multimodal AI speech analysis as a non-invasive early screening tool.Reinforces using acoustic and linguistic metrics to detect subtle speech and fluency changes in early MCI, using picture description tasks for cognitive assessment </span></p></td></tr><tr class="c4"><td class="c74" colspan="1" rowspan="1"><p class="c2"><span class="c21">(Ding et al., 2024)</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">Detection of Mild Cognitive Impairment From Non-Semantic,</span></p><p class="c2"><span class="c21">Acoustic Voice Features: The Framingham Heart Study</span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c21">OpenSMILE feature set used low level descriptors such as descriptors included energy, spectral, cepstral, and</span></p><p class="c2"><span class="c21">voicing-related features</span></p><p class="c2 c13"><span class="c21"></span></p></td><td class="c66" colspan="1" rowspan="1"><p class="c2"><span class="c21">The most important acoustic feature for MCI classification was the number of filled pauses</span></p><p class="c2"><span class="c21">(importance score=0.09, P=3.10E&ndash;08). </span></p><p class="c2 c13"><span class="c21"></span></p><p class="c2 c13"><span class="c21"></span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c21">Determines vocal features potentially indicative of MCI and the potential of monitoring changes to nonsemantic and acoustic features.</span></p></td></tr><tr class="c4"><td class="c74" colspan="1" rowspan="1"><p class="c2"><span class="c21">(Subramanian et al., 2024b)</span></p></td><td class="c64" colspan="1" rowspan="1"><p class="c2"><span class="c21">Detecting Mild Cognitive Impairment using Vocal Biomarkers from</span></p><p class="c2"><span class="c21">Spontaneous Speech</span></p><p class="c2 c13"><span class="c21"></span></p></td><td class="c6" colspan="1" rowspan="1"><p class="c2"><span class="c21">Using start and end times of spoken word syllables, syllable duration, pause duration, filler and word repetition ratios were recorded.</span></p><p class="c2 c13"><span class="c21"></span></p><p class="c2"><span class="c21">Prosody features captured aspects of modulations and </span></p><p class="c2"><span class="c21">rhythm of </span></p><p class="c2"><span class="c21">acoustic energy patterns.</span></p></td><td class="c66" colspan="1" rowspan="1"><p class="c2"><span class="c21">The developed model is able to determine MCI from free speech and from picture description tasks</span></p></td><td class="c31" colspan="1" rowspan="1"><p class="c2"><span class="c21">Determines vocal features potentially indicative of MCI</span></p></td></tr></table><p class="c7 c14"><span class="c10">Table 4. Summary of Speech-Related Studies on MCI Detection</span><span class="c40 c9 c10">&nbsp;</span></p><p class="c20 c8"><span class="c9 c10 c40"></span></p><p class="c20"><span>Although extensive literature exists on speech-based detection of MCI, the broader research programme aims to move away from speech-dependent assessments, due to concerns about patient privacy and the intrusive nature of open-ended vocal data collection. As such, many speech tests performed in industry and prior studies are unsuitable for adoption within the project&rsquo;s future community-based screening framework.</span><span class="c0">&nbsp;</span></p><p class="c20 c13"><span class="c0"></span></p><p class="c20"><span>Software developed to detect MCI by established companies such as Canary Speech and Winterlight Labs confirms that vocal attributes can be extracted and used to detect </span><span>MCI</span><span>.</span></p><hr style="page-break-before:always;display:none;"><h2 class="c14 c91" id="h.b0812e27ym0y"><span class="c40 c9 c54"></span></h2><h2 class="c36 c14" id="h.7d0gvrxn9ghc"><span class="c9">3.2 Data Collection Methods</span></h2><p class="c20 c8"><span class="c0"></span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 246.49px; height: 165.75px;"><img alt="" src="images/image27.png" style="width: 246.49px; height: 165.75px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="Screenshot 2025-10-27 010935.png"></span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c14"><span class="c37 c10">Figure 15: Position of Sensor at Suprasternal Notch </span></p><p class="c7 c14"><span class="c37 c10">(Jeong et al., 2021)</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">The sensor will be placed on the neck near the suprasternal notch (SN) as this position is near the vocal chords, hence enabling the sensor to collect distinct vibrations via the accelerometer. Since the sensor is placed directly on the skin and near the vocal muscles, there will be no leakage of signal from the vocal chords into the air.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 358.67px;"><img alt="" src="images/image9.png" style="width: 601.70px; height: 358.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c0">Figure 16: Choice of Speech Protocol</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span>Fi</span><span>gure 16 </span><span>provides a summary of speech tests used in industry. Our aim is to</span><span>&nbsp;capture vocal data while keeping the signal clear, consistent, and private across subjects</span><span>. </span><span>Tasks such as picture descriptions, open-ended questions, and everyday speech depend heavily on context, making the data less consistent and</span><span>&nbsp;harder to analyse due to privacy.</span><span>&nbsp;As open-ended speech is difficult to standardise, it is difficult to compare speech patterns between patients. Backward counting was tested, and it imposes a higher cognitive demand because it engages several executive functions simultaneously.</span><span class="c0">&nbsp;It requires individuals to hold numbers in working memory and update continuously. Furthermore, other tasks such as counting backwards, pose challenges for sensor-based analysis as accelerometer signals require consistent timing while spontaneous speech varies in pitch and pacing. Unlike structured tasks, which offer consistent and comparable data across participants, open tasks lack repeatability.</span></p><p class="c20 c13"><span class="c0"></span></p><p class="c20"><span class="c0">Since accelerometer data captures physical vibrations such as pauses and fillers, standardisation is important. From our review, we found that the caterpillar task gives the most consistent and reliable signals for analysis.</span></p><hr style="page-break-before:always;display:none;"><h2 class="c1" id="h.o6qlqkgf7h6d"><span class="c40 c9 c54"></span></h2><h2 class="c36 c14" id="h.535tcq8vlbn6"><span class="c9">3.3 Speech Testing - Caterpillar Text</span></h2><p class="c20 c14"><span class="c0">Since the focus is to determine MCI individuals based on vocal characteristics of speech rather than memory and comprehension demands, a constrained fixed passage task was designated as the testing protocol.</span></p><p class="c2 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 485.50px; height: 266.60px;"><img alt="" src="images/image26.png" style="width: 485.50px; height: 266.60px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c37 c10">Figure 17 : The Caterpillar Passage (Annotated)</span></p><p class="c8 c94"><span class="c40 c38 c54"></span></p><p class="c20 c14"><span class="c0">The caterpillar passage was specifically developed for a comprehensive assessment of motor-speech disorders. A relatively low verbal complexity and reading level (Flesh-Kincaid Reading level of 5.0) ensures the passage can be read by most individuals. Other passage attributes encompass word repetition, enabling analysis of inconsistencies within repeated words; multiple intonations, enabling analysis of prosodic features such as fundamental frequency; along with a comprehensive inclusion of all English phonemes. Through vocal data collected from this moderate length passage, further features such as speech rate and number of pauses can be measured (Patel et al., 2013). </span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">These extractable vibrational vocal speech features make the Caterpillar Passage ideal for accelerometer-based mechanoacoustic data collection and analysis for individuals presenting with MCI. &nbsp;</span></p><p class="c20 c8"><span class="c60 c96 c54 c107"></span></p><p class="c20 c14"><span>Vocal speech data via reading the caterpillar passage was collected on healthy individuals for further analysis at a sampling rate of 400Hz.</span></p><hr style="page-break-before:always;display:none;"><h2 class="c1" id="h.p1yqg33lkdvl"><span class="c40 c9 c54"></span></h2><h2 class="c36 c14" id="h.1qgan1g3o6u9"><span class="c9">3.4 Speech Data Analysis</span></h2><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 457.50px; height: 252.80px;"><img alt="" src="images/image11.png" style="width: 457.50px; height: 252.80px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span class="c10 c37">Figure 18: Vocal Data Processing Flow</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span>Figure 18 summarises the overall method used to process vocal data. Using the Metabase sensor, we collect 3 axis accelerometer data. This captures raw vocal vibrational data in the x, y, and z axes at the suprasternal notch. &nbsp;To analyse the various features, it is important to first convert the 3 axis vector to 1 axis by performing Euclidean normalisation (Matic et al., 2012), as reflected in </span><span>Figure 19</span><span class="c0">. This provides an orientation independent representation of the captured vocal vibrational magnitude.</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 331.85px; height: 31.89px;"><img alt="" src="images/image14.png" style="width: 331.85px; height: 31.89px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c37 c10">Figure 19: Euclidean Normalisation </span></p><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span>A bandpass filter is proposed to filter noise from the raw signal with cutoff frequencies between 100Hz to 200Hz, which are the vocal vibrational frequencies of both male and female, respectively. The built-in Python Butterworth filter function was used to perform the filtering of noise. Using the filtered signal, features such as pauses and speech rate are extractable (</span><span>Subramanian et al., 2024; Huang et al., 2024b</span><span class="c0">).</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">The Nyquist-Shannon frequency sampling theorem suggests that a sampling frequency has to be twice that of the analog frequency being analysed, for an accurate reconstruction of analog to digital signals (Lai, 2003). Hence, a 400 Hz sampling frequency rate was proposed.</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 501.63px; height: 277.50px;"><img alt="" src="images/image18.png" style="width: 501.63px; height: 277.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c37 c10">Figure 20 : Vocal data (caterpillar passage, healthy individual), accelerometer</span></p><p class="c7 c8"><span class="c37 c10"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: -0.00px 0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 475.50px; height: 252.76px;"><img alt="" src="images/image6.png" style="width: 475.50px; height: 252.76px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c37 c10">Figure 21 : Vocal data (caterpillar passage, healthy individual), FFT</span></p><p class="c7 c8"><span class="c37 c10"></span></p><p class="c20 c14"><span>This raw signal in Figure 20 undergoes Fast Fourier Transformation (FFT) to Figure 21. Through processing vocal accelerometer signals in the time domain (</span><span>Figure 20) and </span><span>Conversion of the vocal signal to a frequency time domain (Figure 21), quantitative analysis of significant vocal vibrational features such as temporal, fundamental frequency, jitter and shimmer in speech (Huang et al., 2024b) can be extracted and may be indicative of MCI.</span></p><p class="c7 c8"><span class="c37 c10"></span></p><p class="c20 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 520.50px; height: 178.98px;"><img alt="" src="images/image1.png" style="width: 520.50px; height: 178.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c37 c10">Figure 22: Quantitative parameters (Huang et al., 2024b)</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span>Additional processing is commonly performed to convert vocal data to a Mel Frequency Plot, a visual representation of vocal data. This enables further differentiation between individuals with MCI and healthy individuals through the identification of features such as Mel frequency coefficients (</span><span>Huang et al., 2024b; Dubey et al., 2020; Kwon et al., 2024</span><span class="c0">). </span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span class="c0">To narrow down on suitable and informative features that are potentially indicative of MCI, the proposed idea is as follows. Collect vocal recordings of MCI patients performing constrained reading tasks from established sources and recreate/emulate the vocal recordings to collect the accelerometer data. Perform the same constrained reading task on healthy individuals and collect vocal accelerometer data for comparison. Once we have both datasets, we will compare to identify measurable acoustic differences to narrow down the parameters along with the thresholds.</span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 572.50px; height: 103.66px;"><img alt="" src="images/image24.png" style="width: 572.50px; height: 103.66px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c14"><span class="c37 c10">Figure 23 : Vocal Data Collection and Analysis</span></p><p class="c2 c8"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><h2 class="c1" id="h.mwmb72slydic"><span class="c40 c9 c54"></span></h2><h2 class="c36 c14" id="h.jv0ndesodpcq"><span class="c9">3.5 Evaluation</span></h2><p class="c2 c8"><span class="c0"></span></p><p class="c20 c14"><span>Pratt is a software used to analyse recorded voice samples. After completing the setup and recording the voice task the audio is loaded into PRAAT, which displays the spectrogram.</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>Pratt </span><span>extracts key acoustic measures such as fundamental frequency (pitch), jitter, shimmer, harmonics-to-noise ratio and mean loudness, and also generates the cepstral peak prominence (CPP), an important indicator of voice quality</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c20 c14"><span>Overall, PRAAT converts simple voice recordings into meaningful acoustic features though an accessible workflow</span></p><p class="c20 c8"><span class="c0"></span></p><p class="c2 c14"><span>Due to the availability of a multitude of speech analysis and synthesis options, we will look into using Praat to evaluate the vocal speech data. </span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 582.50px; height: 264.16px;"><img alt="" src="images/image12.png" style="width: 582.50px; height: 264.16px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c7 c14"><span class="c37 c10">Figure 24: Features of Praat Software </span></p><p class="c7 c14"><span class="c10">(Mathad et al., 2020)</span></p><h1 class="c81 c14 c70" id="h.z255ocf2dar8"><span class="c40 c67 c54"></span></h1><hr style="page-break-before:always;display:none;"><h1 class="c29 c14 c70" id="h.6hgiy0n8lxzr"><span class="c24"></span></h1><h1 class="c29 c14" id="h.yse2nxxdy54p"><span class="c67">4. Summary</span></h1><p class="c2 c8"><span class="c60 c9 c54 c62"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 601.70px; height: 413.33px;"><img alt="" src="images/image22.png" style="width: 601.70px; height: 413.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c13"><span class="c27 c10 c33"></span></p><p class="c7"><span class="c27 c10 c33">Figure 25: Overview of Speech and Gait Processes</span></p><p class="c2 c8"><span class="c60 c9 c54 c62"></span></p><p class="c56"><span>The gait module uses the research sensor to record</span><span class="c0">&nbsp;motion data, detect gait events, and extract features such as stride time and step variability. The data analysis method was validated against Vicon. The speech module uses the same sensor to capture vibration data, process it through filtering and spectrograms, and extract features such as pause duration and speech rate. Collectively, results from both modules will be used to screen for MCI, as seen in Figure 25.</span></p><p class="c2 c8"><span class="c40 c9 c54"></span></p><p class="c20 c14"><span class="c0">Gait and speech are regulated by brain regions that are responsible for movement, coordination, and cognition. When these areas begin to deteriorate, subtle changes in walking and speaking often surface before memory problems become noticeable. Our project aims to prove that simple, low-cost sensors, coupled with well-designed tasks and clear data analysis, can detect early signs of MCI, making large-scale screening for MCI more accessible.</span></p><hr style="page-break-before:always;display:none;"><p class="c20 c8"><span class="c40 c67 c54"></span></p><h1 class="c14 c29" id="h.tbolmsxh67hs"><span class="c24">5. Future Project Plan</span></h1><p class="c2 c8 c68"><span class="c0"></span></p><p class="c7 c13"><span class="c27 c10 c33"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 597.45px; height: 360.00px;"><img alt="" src="images/image10.png" style="width: 602.00px; height: 360.00px; margin-left: -4.55px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7"><span class="c27 c10 c33">Figure 26: Project Plan for Semester 2 </span></p><p class="c2 c8"><span class="c0"></span></p><p class="c56"><span>In S</span><span>emester 2, work will involve expanding to patient testing, refining algorithms to optimise existing Python scripts, and standardising data analysis processes for gait and speech. Patient data collection will also begin at regular intervals, whilst improving the data analysis code.</span></p><p class="c2 c8"><span class="c82 c76"><br></span><hr style="page-break-before:always;display:none;"></p><h1 class="c81 c14" id="h.wl0d9y6j6atq"><span class="c9">6. References</span></h1><p class="c2 c14"><span class="c48 c33">Eshkoor, S. A., Hamid, T. A., Mun, C. Y., &amp; Ng, C. K. (2015). Mild cognitive impairment and its management in older people. </span><span class="c10 c48 c33">Clinical interventions in aging</span><span class="c48 c33">, </span><span class="c10 c48 c33">10</span><span class="c48 c33">, 687&ndash;693. </span><span class="c48 c33"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.2147/CIA.S73922&amp;sa=D&amp;source=editors&amp;ust=1763177523842028&amp;usg=AOvVaw23PT7yQD5bHTCmMezR-4Ga">https://doi.org/10.2147/CIA.S73922</a></span></p><p class="c2 c8"><span class="c23 c38 c33"></span></p><p class="c2 c14"><span class="c33 c48">Cipriani, G., Danti, S., Picchi, L., Nuti, A., &amp; Fiorino, M. D. (2020). Daily functioning and dementia. </span><span class="c10 c48 c33">Dementia &amp; neuropsychologia</span><span class="c48 c33">, </span><span class="c10 c48 c33">14</span><span class="c23 c38 c33">(2), 93&ndash;102. https://doi.org/10.1590/1980-57642020dn14-020001</span></p><p class="c2 c8"><span class="c23 c38 c33"></span></p><p class="c2 c14"><span class="c48 c33">Weiner, M. W., Veitch, D. P., Aisen, P. S., Beckett, L. A., Cairns, N. J., Green, R. C., Harvey, D., Jack, C. R., Jagust, W., Liu, E., Morris, J. C., Petersen, R. C., Saykin, A. J., Schmidt, M. E., Shaw, L., Shen, L., Siuciak, J. A., Soares, H., Toga, A. W., Trojanowski, J. Q., &hellip; Alzheimer&#39;s Disease Neuroimaging Initiative (2013). The Alzheimer&#39;s Disease Neuroimaging Initiative: a review of papers published since its inception. </span><span class="c10 c48 c33">Alzheimer&#39;s &amp; dementia : the journal of the Alzheimer&#39;s Association</span><span class="c48 c33">, </span><span class="c10 c48 c33">9</span><span class="c23 c33 c38">(5), e111&ndash;e194. https://doi.org/10.1016/j.jalz.2013.05.1769</span></p><p class="c2 c8"><span class="c38 c33 c54 c89"></span></p><p class="c2 c14"><span class="c52 c45">Dodge, H. H., Mattek, N. C., Austin, D., Hayes, T. L., &amp; Kaye, J. A. (2012). In-home walking speeds and variability trajectories associated with mild cognitive impairment. Neurology, 78(24), 1946&ndash;1952. </span><span class="c52 c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1212/WNL.0b013e318259e1de&amp;sa=D&amp;source=editors&amp;ust=1763177523844109&amp;usg=AOvVaw1yOpef1s8xaLQpnN640zry">https://doi.org/10.1212/WNL.0b013e318259e1de</a></span></p><p class="c2 c8"><span class="c60 c38 c45 c54"></span></p><p class="c2 c8"><span class="c60 c38 c45 c54"></span></p><p class="c2 c14"><span class="c45">Abhayasinghe, N., &amp; Murray, I. (2014). Human GaIT phase recognition based on thigh movement computed using IMUs. 2014 IEEE Ninth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP), 1, 1&ndash;4. </span><span class="c82 c86"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1109/issnip.2014.6827604&amp;sa=D&amp;source=editors&amp;ust=1763177523844765&amp;usg=AOvVaw0QDC_AmtH2JcUoPvHNEytd">https://doi.org/10.1109/issnip.2014.6827604</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14"><span class="c45">Panebianco, G. P., Bisi, M. C., Stagni, R., &amp; Fantozzi, S. (2020). Timing estimation for gait in water from inertial sensor measurements: Analysis of the performance of 17 algorithms. Computer Methods and Programs in Biomedicine, 197, 105703. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1016/j.cmpb.2020.105703&amp;sa=D&amp;source=editors&amp;ust=1763177523845455&amp;usg=AOvVaw3bSWTMuCR6dDGZtwYXghfH">https://doi.org/10.1016/j.cmpb.2020.105703</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14"><span class="c33 c45">Vargas-Valencia, L., Elias, A., Rocon, E., Bastos-Filho, T., &amp; Frizera, A. (2016). An IMU-to-Body alignment method applied to human GAIT analysis. Sensors, 16(12), 2090. </span><span class="c19 c33"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.3390/s16122090&amp;sa=D&amp;source=editors&amp;ust=1763177523845851&amp;usg=AOvVaw1MG3h_ehDDuKtry-jlbzFB">https://doi.org/10.3390/s16122090</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14"><span class="c45">Trojaniello, D., Cereatti, A., Pelosin, E., Avanzino, L., Mirelman, A., Hausdorff, J. M., &amp; Della Croce, U. (2014). Estimation of step-by-step spatio-temporal parameters of normal and impaired gait using shank-mounted magneto-inertial sensors: application to elderly, hemiparetic, parkinsonian and choreic gait. Journal of NeuroEngineering and Rehabilitation, 11(1), 152. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1186/1743-0003-11-152&amp;sa=D&amp;source=editors&amp;ust=1763177523846579&amp;usg=AOvVaw2ssRuFl0HCMh1sc99iEdnk">https://doi.org/10.1186/1743-0003-11-152</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14 c43"><span class="c45">Canonico, M., Desimoni, F., Ferrero, A., Grassi, P. A., Irwin, C., Campani, D., Molin, A. D., Panella, M., &amp; Magistrelli, L. (2023b). GAIT Monitoring and Analysis: A Mathematical approach. Sensors, 23(18), 7743. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.3390/s23187743&amp;sa=D&amp;source=editors&amp;ust=1763177523846983&amp;usg=AOvVaw3Mc5de0aiMghg6bxRUrNoe">https://doi.org/10.3390/s23187743</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14 c43"><span class="c45">Vicon Motion Systems. (2025, October 23). Award winning motion capture Systems | VICON. Vicon. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://www.vicon.com/&amp;sa=D&amp;source=editors&amp;ust=1763177523847260&amp;usg=AOvVaw2Q2PMd9GrGqoA2iHuVjVNP">https://www.vicon.com/</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14 c43"><span class="c45">Vicon Motion Systems. (2019, November 20). What is the Plug-in Gait &ldquo;Progression Frame&rdquo; and how is it used? | Vicon. Vicon. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://www.vicon.com/support/faqs/what-is-the-plug-in-gait-progression-frame-and-how-is-it-used/&amp;sa=D&amp;source=editors&amp;ust=1763177523847679&amp;usg=AOvVaw2Y5PUq6GNPXARiH8Lw-eZK">https://www.vicon.com/support/faqs/what-is-the-plug-in-gait-progression-frame-and-how-is-it-used/</a></span></p><p class="c2 c8 c43"><span class="c60 c38 c45 c54"></span></p><p class="c2 c14 c43"><span class="c45">Jeong, H., Lee, J. Y., Lee, K., Kang, Y. J., Kim, J., Avila, R., Tzavelis, A., Kim, J., Ryu, H., Kwak, S. S., Kim, J. U., Banks, A., Jang, H., Chang, J., Li, S., Mummidisetty, C. K., Park, Y., Nappi, S., Chun, K. S., . . . Rogers, J. A. (2021). Differential cardiopulmonary monitoring system for artifact-canceled physiological tracking of athletes, workers, and COVID-19 patients. Science Advances, 7(20). </span><span class="c82 c86"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1126/sciadv.abg3092&amp;sa=D&amp;source=editors&amp;ust=1763177523848438&amp;usg=AOvVaw3iTo5LAEtaxR40GN9e903j">https://doi.org/10.1126/sciadv.abg3092</a></span></p><p class="c2 c8 c43"><span class="c60 c38 c19 c54"></span></p><p class="c2 c14 c43"><span class="c45">Ding, H., Lister, A., Karjadi, C., Au, R., Lin, H., Bischoff, B., &amp; Hwang, P. H. (2024). Detection of mild Cognitive Impairment from Non-Semantic, Acoustic Voice features: The Framingham Heart Study. JMIR Aging, 7, e55126. </span><span class="c82 c86"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.2196/55126&amp;sa=D&amp;source=editors&amp;ust=1763177523849089&amp;usg=AOvVaw0WdMYotmb3yPVgE1DCvwuv">https://doi.org/10.2196/55126</a></span></p><p class="c2 c8 c43"><span class="c60 c38 c19 c54"></span></p><p class="c2 c14 c43"><span class="c45">Patel, R., Connaghan, K., Franco, D., Edsall, E., Forgit, D., Olsen, L., Ramage, L., Tyler, E., &amp; Russell, S. (2013). &ldquo;The Caterpillar&rdquo;: A novel reading passage for assessment of Motor Speech Disorders. American Journal of Speech-Language Pathology, 22(1), 1&ndash;9. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1044/1058-0360(2012/11-0134&amp;sa=D&amp;source=editors&amp;ust=1763177523849650&amp;usg=AOvVaw00xOhx2igF947dRIrupam2">https://doi.org/10.1044/1058-0360(2012/11-0134</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14 c43"><span class="c45">Matic, A., Osmani, V., &amp; Mayora, O. (2012). Speech activity detection using accelerometer. Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference, 2012, 2112&ndash;2115. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1109/embc.2012.6346377&amp;sa=D&amp;source=editors&amp;ust=1763177523850187&amp;usg=AOvVaw1JLwpSqrWzwadQlE1iI26c">https://doi.org/10.1109/embc.2012.6346377</a></span></p><p class="c2 c14 c43"><span class="c45">Lai, E. (2003). Converting analog to digital signals and vice versa. In Elsevier eBooks (pp. 14&ndash;49). </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1016/b978-075065798-3/50002-3&amp;sa=D&amp;source=editors&amp;ust=1763177523850481&amp;usg=AOvVaw1BIigTkj15nyERUgS2fBN0">https://doi.org/10.1016/b978-075065798-3/50002-3</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c87"><span class="c26">L</span><span class="c48">anzi, A. M. et al. (2023). </span><span class="c53"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1044/2022_AJSLP-22-00281&amp;sa=D&amp;source=editors&amp;ust=1763177523850804&amp;usg=AOvVaw2T2qfD0Du4lF_hkHEJdqx-">https://doi.org/10.1044/2022_AJSLP-22-00281</a></span><span class="c53">&nbsp; </span><span class="c44 c9">(Narrative)</span></p><p class="c87"><span class="c48">Mart&iacute;nez-Nicol&aacute;s, I., Llorente, T. E., Mart&iacute;nez-S&aacute;nchez, F., &amp; Meil&aacute;n, J. J. G. (2021).</span><span class="c53"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.3389/fpsyg.2021.620251&amp;sa=D&amp;source=editors&amp;ust=1763177523851098&amp;usg=AOvVaw3vTwYRXykeRrcOTt1d5G3G">https://doi.org/10.3389/fpsyg.2021.620251</a></span><span class="c82 c48">&nbsp;(</span><span class="c9 c44">Open ended)</span></p><p class="c87"><span class="c48">Chen, Y., Hartsuiker, R. J., &amp; Pistono, A. (2024)</span><span class="c61">.</span><span class="c53"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1080/02687038.2024.2358556&amp;sa=D&amp;source=editors&amp;ust=1763177523851362&amp;usg=AOvVaw0VMOjIsipxXjmCvBojhNkt">https://doi.org/10.1080/02687038.2024.2358556</a></span><span class="c48">&nbsp;</span><span class="c48 c76">(Narrative recall)</span></p><p class="c58"><span class="c48">Saunders, S., Haider, F., Ritchie, C. W., Muniz Terrera, G., &amp; Luz, S. (2024) </span><span class="c53"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1136/bmjopen-2023-082388&amp;sa=D&amp;source=editors&amp;ust=1763177523851664&amp;usg=AOvVaw3BfUoqfkieGa_-LNgeB8en">https://doi.org/10.1136/bmjopen-2023-082388</a></span><span class="c76 c61">&nbsp;</span><span class="c9 c23">(Daily speech)</span></p><p class="c58"><span class="c48">Croisile, B., Ska, B., Brabant, M. J., Duchene, A., Lepage, Y., Aimard, G., &amp; Trillet, M. (1996).</span><span class="c53"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1006/brln.1996.0033&amp;sa=D&amp;source=editors&amp;ust=1763177523851952&amp;usg=AOvVaw19BJgJIuEgJQJMurv9PHYd">https://doi.org/10.1006/brln.1996.0033</a></span><span class="c48">&nbsp;</span><span class="c23 c9">(Cookie pic description)</span></p><p class="c58"><span class="c48">Mart&iacute;nez-Nicol&aacute;s et al. (2023)</span><span class="c53"><a class="c34" href="https://www.google.com/url?q=https://www.nature.com/articles/s41598-023-36804-y&amp;sa=D&amp;source=editors&amp;ust=1763177523852407&amp;usg=AOvVaw076JTG9LcQzjoZKNy0wZvL">https://www.nature.com/articles/s41598-023-36804-y</a></span><span class="c61">&nbsp;</span><span class="c48 c76">(Don Quixote)</span></p><p class="c2 c14 c43"><span class="c45">Roberts, L. (2024, January 17). Understanding the Mel Spectrogram. Medium. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53&amp;sa=D&amp;source=editors&amp;ust=1763177523853135&amp;usg=AOvVaw3CZ8Qw9vXBBPeD8BBqZIBW">https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14 c43"><span class="c45">Dubey, S., Mahnan, A., &amp; Konczak, J. (2020). Real-Time Voice Activity Detection Using Neck-Mounted Accelerometers for Controlling a Wearable Vibration Device to Treat Speech Impairment. Proceedings of the 2020 Design of Medical Devices Conference. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1115/dmd2020-9081&amp;sa=D&amp;source=editors&amp;ust=1763177523853778&amp;usg=AOvVaw0mhglrMu2TN9_MnrZ9b1tu">https://doi.org/10.1115/dmd2020-9081</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14 c43"><span class="c45">Kwon, J., Hwang, J., Sung, J. E., &amp; Im, C. (2024). Speech synthesis from three-axis accelerometer signals using conformer-based deep neural network. Computers in Biology and Medicine, 182, 109090. </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1016/j.compbiomed.2024.109090&amp;sa=D&amp;source=editors&amp;ust=1763177523854309&amp;usg=AOvVaw0JUgQAxlShXMqtopy14cFM">https://doi.org/10.1016/j.compbiomed.2024.109090</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14 c43"><span class="c45">Raju, A. R., Kalathinathan, A., &amp; Vally, M. (2022). SIGNIFICANCE OF PRATT SOFTWARE: UNDERSTANDING ITS PHONOLOGICAL CHARACTERISTICS AND PROSODIC FEATURES. Zenodo (CERN European Organization for Nuclear Research). </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.5281/zenodo.7327004&amp;sa=D&amp;source=editors&amp;ust=1763177523854823&amp;usg=AOvVaw1ivG-OjNfHe7kIftm-VmzJ">https://doi.org/10.5281/zenodo.7327004</a></span></p><p class="c2 c8"><span class="c0"></span></p><p class="c2 c14 c43"><span class="c45">Praat: doing Phonetics by Computer. (n.d.). </span><span class="c19"><a class="c34" href="https://www.google.com/url?q=https://www.fon.hum.uva.nl/praat/&amp;sa=D&amp;source=editors&amp;ust=1763177523855114&amp;usg=AOvVaw165EehJANwwiDSJnQZ1psX">https://www.fon.hum.uva.nl/praat/</a></span></p><p class="c2 c8"><span class="c60 c38 c77 c54"></span></p><p class="c20 c8"><span class="c60 c9 c26 c54"></span></p><p class="c20 c14"><span class="c26">Prisco, G., Pirozzi, M. A., Santone, A., Esposito, F., Cesarelli, M., Amato, F., &amp; Donisi, L. (2024). Validity of Wearable Inertial Sensors for Gait Analysis: A Systematic Review. </span><span class="c10 c26">Diagnostics (Basel, Switzerland)</span><span class="c26">, </span><span class="c10 c26">15</span><span class="c26">(1), 36. </span><span class="c18">&nbsp;https://doi.org/10.3390/diagnostics15010036</span></p><p class="c20 c8 c68"><span class="c22"></span></p><p class="c20 c14"><span class="c26">He, Y., Chen, Y., Tang, L., Chen, J., Tang, J., Yang, X., Su, S., Zhao, C., &amp; Xiao, N. (2024). Accuracy validation of a wearable IMU-based gait analysis in healthy female. </span><span class="c10 c26">BMC sports science, medicine &amp; rehabilitation</span><span class="c26">, </span><span class="c10 c26">16</span><span class="c26">(1), 2. </span><span class="c18">https://doi.org/10.1186/s13102-023-00792-3</span></p><p class="c20 c8"><span class="c60 c9 c26 c54"></span></p><p class="c20 c8 c68"><span class="c22"></span></p><p class="c20 c14"><span class="c26">Montero-Odasso, M., Muir, S. W., &amp; Speechley, M. (2012). Dual-task complexity affects gait in people with mild cognitive impairment: the interplay between gait variability, dual tasking, and risk of falls. </span><span class="c10 c26">Archives of physical medicine and rehabilitation</span><span class="c26">, </span><span class="c10 c26">93</span><span class="c26">(2), 293&ndash;299. </span><span class="c53"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1016/j.apmr.2011.08.026&amp;sa=D&amp;source=editors&amp;ust=1763177523857160&amp;usg=AOvVaw3SL9tE6gSSwcdgdmcA4TEx">https://doi.org/10.1016/j.apmr.2011.08.026</a></span></p><p class="c20 c8 c68"><span class="c22"></span></p><p class="c20 c14"><span class="c26">He, Y., Chen, Y., Tang, L. </span><span class="c10 c26">et al.</span><span class="c26">&nbsp;Accuracy validation of a wearable IMU-based gait analysis in healthy female. </span><span class="c10 c26">BMC Sports Sci Med Rehabil</span><span class="c26">&nbsp;16, 2 (2024). </span><span class="c18">https://doi.org/10.1186/s13102-023-00792-3</span></p><p class="c20 c8 c68"><span class="c22"></span></p><p class="c20 c14"><span class="c26">Beauchet, O., Launay, C.P., Sejdi&#263;, E. </span><span class="c10 c26">et al.</span><span class="c26">&nbsp;Motor imagery of gait: a new way to detect mild cognitive impairment?. </span><span class="c10 c26">J NeuroEngineering Rehabil</span><span class="c26">&nbsp;11, 66 (2014). </span><span class="c18">https://doi.org/10.1186/1743-0003-11-66</span></p><p class="c20 c8 c68"><span class="c22"></span></p><p class="c20 c14"><span class="c26">&nbsp;F Ram&iacute;rez &amp; G Guti&eacute;rrez &mdash; </span><span class="c10 c26">&ldquo;Dual-Task Gait as a Predictive Tool for Cognitive Impairment in Older Adults: A Systematic Review&rdquo;</span><span class="c26">&nbsp;(2021)</span><span class="c61">Link:</span><span class="c53"><a class="c34" href="https://www.google.com/url?q=https://www.frontiersin.org/journals/aging-neuroscience/articles/10.3389/fnagi.2021.769462/pdf?utm_source%3Dchatgpt.com&amp;sa=D&amp;source=editors&amp;ust=1763177523858668&amp;usg=AOvVaw2wFWxVfhF4cRUFvqdY4FPr">&nbsp;Frontiers in Aging Neuroscience</a></span></p><p class="c20 c8"><span class="c18"></span></p><p class="c20 c8"><span class="c18"></span></p><p class="c20 c14"><span class="c52 c45">Dodge, H. H., Mattek, N. C., Austin, D., Hayes, T. L., &amp; Kaye, J. A. (2012). In-home walking speeds and variability trajectories associated with mild cognitive impairment. Neurology, 78(24), 1946&ndash;1952. </span><span class="c52 c19"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1212/WNL.0b013e318259e1de&amp;sa=D&amp;source=editors&amp;ust=1763177523859172&amp;usg=AOvVaw0Kgq3wiLNuD7QOwvEihIH5">https://doi.org/10.1212/WNL.0b013e318259e1de</a></span></p><p class="c20 c8"><span class="c18"></span></p><p class="c14 c56"><span class="c52 c45">(2024, June 18). </span><span class="c52 c45">New Study: Unplanned Healthcare Utilisation Increases Cost Burden for Patients with Cognitive Impairment</span><span class="c59 c38 c45 c54">. Retrieved November 14, 2025, from https://www.duke-nus.edu.sg/newshub/media-releases/unplanned-healthcare-utilisation-increases-cost-burden-for-patients-with-cognitive-impairment. </span></p><p class="c56 c14"><span class="c59 c38 c45 c54">Woo, L. L., Thompson, C. L., &amp; Magadi, H. (2017). Monetary cost of family caregiving for people with dementia in Singapore. Archives of Gerontology and Geriatrics, 71, 59&ndash;65. https://doi.org/10.1016/j.archger.2017.03.006</span></p><p class="c20 c14"><span class="c45 c52">Mathad, V. C., Chapman, K., Liss, J., Scherer, N., &amp; Berisha, V. (2020). Deep Learning Based Prediction of Hypernasality for Clinical Applications. 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 6554&ndash;6558. </span><span class="c52 c45"><a class="c34" href="https://www.google.com/url?q=https://doi.org/10.1109/icassp40776.2020.9054041&amp;sa=D&amp;source=editors&amp;ust=1763177523860792&amp;usg=AOvVaw14coS9WpRYMQWsAVc8SDVg">https://doi.org/10.1109/icassp40776.2020.9054041</a></span></p><div><p class="c8 c71"><span class="c0"></span></p></div></body></html>